{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6dae1c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print(...)? (NetFlow.py, line 300)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3460\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 1\u001b[0;36m\n\u001b[0;31m    import toolbox.NetFlow as nf\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m~/prac/Mobihoc-18-STN-mobile-traffic-forecasting/toolbox/NetFlow.py:300\u001b[0;36m\u001b[0m\n\u001b[0;31m    print 'prediction:', np.mean(np.mean(input_source, axis=0), axis=0)[-1], 'GT:', dataset[:, :, input_size[-1] + frame-1].mean()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(...)?\n"
     ]
    }
   ],
   "source": [
    "import toolbox.NetFlow as nf\n",
    "from toolbox import DataProvider\n",
    "from toolbox import LayerExtension\n",
    "import tensorflow as tf\n",
    "import tensorlayer as tl\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "\n",
    "def get_arguments():\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='Train a STN\\\n",
    "                                     for mobile traffic forecasting')\n",
    "    parser.add_argument('--datadir',\n",
    "                        type=str,\n",
    "                        default='./data/',\n",
    "                        help='this is the directory of the training samples')\n",
    "    parser.add_argument('--batchsize',\n",
    "                        type=int,\n",
    "                        default=100,\n",
    "                        help='The batch size of training examples')\n",
    "    parser.add_argument('--epoch',\n",
    "                        type=int,\n",
    "                        default=50,\n",
    "                        help='The number of epoches.')\n",
    "    parser.add_argument('--save_model',\n",
    "                        type=str,\n",
    "                        default='stn.npz',\n",
    "                        help='Save the learnt model: \\\n",
    "                            0 -- not to save the learnt model parameters;\\\n",
    "                            n (n>0) -- to save the model params every n steps;\\\n",
    "                            -1 -- only save the learnt model params at the end of training.\\\n",
    "                             n = 1 by default')\n",
    "    parser.add_argument('--model_path',\n",
    "                        type=str,\n",
    "                        default='./',\n",
    "                        help='Saved model path')\n",
    "    parser.add_argument('--model_name',\n",
    "                        type=str,\n",
    "                        default='stn',\n",
    "                        help='Saved model name')\n",
    "    parser.add_argument('--pre_model_path',\n",
    "                        type=str,\n",
    "                        default=None,\n",
    "                        help='Pretrained model path')\n",
    "    parser.add_argument('--pre_model_name',\n",
    "                        type=str,\n",
    "                        default=None,\n",
    "                        help='Pretrained model name')\n",
    "    parser.add_argument('--mean',\n",
    "                        type=float,\n",
    "                        default=0,\n",
    "                        help='mean value for data normalisation: \\\n",
    "                            (data-mean)/std')\n",
    "    parser.add_argument('--std',\n",
    "                        type=float,\n",
    "                        default=1,\n",
    "                        help='standard deviation value for data normalisation: \\\n",
    "                            (data-mean)/std')\n",
    "    parser.add_argument('--observations',\n",
    "                        type=int,\n",
    "                        default=12,\n",
    "                        help='temporal length of input')\n",
    "    parser.add_argument('--lr',\n",
    "                        type=int,\n",
    "                        default=0.001,\n",
    "                        help='learning rate of the model')\n",
    "    parser.add_argument('--framebatch',\n",
    "                        type=int,\n",
    "                        default=1,\n",
    "                        help=\"maximum frames selected in one mini-batch\")\n",
    "    parser.add_argument('--input_x',\n",
    "                        type=int,\n",
    "                        default=11,\n",
    "                        help=\"spatial length of input of x axis\")\n",
    "    parser.add_argument('--input_y',\n",
    "                        type=int,\n",
    "                        default=11,\n",
    "                        help=\"spatial length of input of y axis\")\n",
    "    parser.add_argument('--pad',\n",
    "                        type=tuple,\n",
    "                        default=(5, 5),\n",
    "                        help=\"2-element or None, the size of padding\")\n",
    "    parser.add_argument('--pad_value',\n",
    "                        type=int,\n",
    "                        default=0,\n",
    "                        help=\"the value of padding\")\n",
    "    parser.add_argument('--prediction_gap',\n",
    "                        type=int,\n",
    "                        default=1,\n",
    "                        help=\"the distance between the last input frame and output frame\")\n",
    "    parser.add_argument('--stride',\n",
    "                        type=tuple,\n",
    "                        default=(1, 1),\n",
    "                        help=\"2-element tuple, the stride of slicing the original snapshot\")\n",
    "    parser.add_argument('--ouroboros_e',\n",
    "                        type=int,\n",
    "                        default=0,\n",
    "                        help=\"The epoch for the ouroboros training scheme\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "args = get_arguments()\n",
    "\n",
    "\n",
    "def load_dataset():\n",
    "\n",
    "    tra_set = np.load(args.datadir + 'milan_tra.npy')\n",
    "    val_set = np.load(args.datadir + 'milan_val.npy')\n",
    "    test_set = np.load(args.datadir + 'milan_test.npy')\n",
    "    print('training set:', tra_set.shape)\n",
    "    print('validation set:', val_set.shape)\n",
    "    print('test set:', test_set.shape)\n",
    "\n",
    "    return tra_set, val_set, test_set\n",
    "\n",
    "tra_set, val_set, test_set = load_dataset()\n",
    "\n",
    "\n",
    "framebatch = args.framebatch\n",
    "stride = args.stride\n",
    "input_size = (args.input_x, args.input_y, args.observations)\n",
    "prediction_gap = args.prediction_gap\n",
    "batchsize = args.batchsize\n",
    "pad = args.pad\n",
    "pad_value = args.pad_value\n",
    "num_epochs = args.epoch\n",
    "learning_rate = args.lr\n",
    "shuffle = True\n",
    "flatten = True\n",
    "output_size = (1,1,1)\n",
    "mean = tra_set.mean()\n",
    "std = tra_set.mean()\n",
    "\n",
    "tra_kwag = {\n",
    "    'inputs':tra_set,\n",
    "    'framebatch': framebatch,\n",
    "    'mean': args.mean,\n",
    "    'std': args.std,\n",
    "    'norm_tar': True}\n",
    "\n",
    "val_kwag = {\n",
    "    'inputs': val_set,\n",
    "    'framebatch': framebatch,\n",
    "    'mean': args.mean,\n",
    "    'std': args.std,\n",
    "    'norm_tar': True}\n",
    "\n",
    "test_kwag = {\n",
    "    'inputs': test_set,\n",
    "    'framebatch': framebatch,\n",
    "    'mean': args.mean,\n",
    "    'std': args.std,\n",
    "    'norm_tar': True}\n",
    "\n",
    "tra_provider = DataProvider.Provider(stride = stride, input_size = input_size,\n",
    "                           output_size = output_size, prediction_gap = prediction_gap,\n",
    "                           batchsize = batchsize, pad = pad, pad_value = pad_value, shuffle = True)\n",
    "\n",
    "val_provider = DataProvider.Provider(stride = (4,4), input_size = input_size,\n",
    "                           output_size = output_size, prediction_gap = prediction_gap,\n",
    "                           batchsize = -1, pad = pad, pad_value = pad_value, shuffle = False)\n",
    "\n",
    "test_provider = DataProvider.Provider(stride = stride, input_size = input_size,\n",
    "                           output_size = output_size, prediction_gap = prediction_gap,\n",
    "                           batchsize = -1, pad = pad, pad_value = pad_value, shuffle = False)\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, args.observations, args.input_x, args.input_y], name='x')\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 1], name='y_')\n",
    "\n",
    "\n",
    "\n",
    "def stn(x, input_x, input_y, observations, reuse=False, name='stn'):\n",
    "\n",
    "    with tf.variable_scope(name, reuse=reuse):\n",
    "        network = tl.layers.InputLayer(x, name='input_layer')\n",
    "        network = tl.layers.ReshapeLayer(network, shape = (-1, observations, input_x, input_y, 1))\n",
    "        conv1 = tl.layers.Conv3dLayer(network, shape = [3, 3 ,3, 1, 3], strides=[1, 1, 1, 1, 1], name = 'conv1')\n",
    "        lstm1 = LayerExtension.ConvRNNLayer(network, cell_shape = (input_x, input_y), cell_fn = ConvRNNCell.BasicConvLSTMCell, n_steps = observations, feature_map = 3,\n",
    "                                             name='convlstm1')\n",
    "        network = tl.layers.ConcatLayer([lstm1, conv1], concat_dim=4)\n",
    "        conv2 = tl.layers.Conv3dLayer(network, shape = [3, 3 ,3, 6, 6], strides=[1, 1, 1, 1, 1], name = 'conv2')\n",
    "        lstm2 = LayerExtension.ConvRNNLayer(network, cell_shape = (input_x, input_y), cell_fn = ConvRNNCell.BasicConvLSTMCell, n_steps = observations, feature_map = 6,\n",
    "                                              name='convlstm2')\n",
    "        network = tl.layers.ConcatLayer([lstm2, conv2], concat_dim=4, name = 'concat2')\n",
    "        conv3 = tl.layers.Conv3dLayer(network, shape = [3, 3 ,3, 12, 12], strides=[1, 1, 1, 1, 1], name = 'conv3')\n",
    "        lstm3 = LayerExtension.ConvRNNLayer(network, cell_shape = (input_x, input_y), cell_fn = ConvRNNCell.BasicConvLSTMCell, n_steps = observations, feature_map = 12,\n",
    "                                              name='convlstm3')\n",
    "        network = tl.layers.ConcatLayer([lstm3, conv3], concat_dim=4, name = 'concat3')\n",
    "        network = tl.layers.FlattenLayer(network)\n",
    "        network = tl.layers.DenseLayer(network, n_units=4096,\n",
    "                                        act = tf.nn.relu ,\n",
    "                                        name='dense1')\n",
    "        network = tl.layers.DenseLayer(network, n_units=1024,\n",
    "                                        act = tf.nn.relu,\n",
    "                                        name='dense2')\n",
    "        network = tl.layers.DenseLayer(network, n_units=1,\n",
    "                                        act = tl.activation.identity,\n",
    "                                        name='output_layer')\n",
    "        return network\n",
    "\n",
    "\n",
    "network = stn(x, args.input_x, args.input_y, args.observations)\n",
    "y = network.outputs\n",
    "cost = tl.cost.mean_squared_error(y, y_)\n",
    "train_params = network.all_params\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=args.lr, beta1=0.9, beta2=0.999,\n",
    "                                  epsilon=1e-08, use_locking=False).minimize(cost, var_list=train_params)\n",
    "\n",
    "# initialize all variables\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "if args.pre_model_path is not None:\n",
    "    load_params = tl.files.load_npz(path=args.pre_model_path, name=args.pre_model_name)\n",
    "    tl.files.assign_params(sess, load_params, network)\n",
    "\n",
    "print ('set done')\n",
    "\n",
    "nf.customfit(sess=sess, network=network, cost=cost, train_op=train_op, tra_provider=tra_provider, x=x, y_=y_, acc=None,\n",
    "             n_epoch=args.epoch, print_freq=1, val_provider=val_provider, save_model=1, tra_kwag=tra_kwag, val_kwag=val_kwag,\n",
    "             save_path=args.model_path + args.model_name, epoch_identifier=None)\n",
    "\n",
    "if args.ouroboros_e > 0:\n",
    "    nf.Ouroborosfit(sess=sess, network=network, cost = cost, train_op=train_op, x=x, y_=y_, dataset = tra_set,\n",
    "                    batchsize=batchsize, input_size=input_size, pad = pad, n_epoch=args.ouroboros_e, mean=mean, std=std, val_provider=val_provider,\n",
    "                    save_model=1, val_kwag=val_kwag ,save_path=args.model_path + args.model_name + '_ots', epoch_identifier=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc451496",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
